# AI Agent Frameworks Comparison Project

A comprehensive evaluation and comparison of leading Python AI agent frameworks across standardized use cases and metrics.

## ğŸ¯ Project Overview

This project systematically compares five leading Python AI agent frameworks:
- **DSPy** - Programming framework for language models [HIGHEST PRIORITY]
- **PocketFlow** - Nested directed graph framework
- **CrewAI** - Multi-agent orchestration framework
- **Google ADK** - Google's Agent Development Kit
- **Pydantic AI** - Type-safe agent framework [LOWEST PRIORITY]

Each framework is evaluated across six standardized use cases with complete isolation to ensure fair comparison.

## ğŸ—ï¸ Architecture

The project uses a **framework-isolated architecture** where each AI framework operates in its own environment with dedicated infrastructure, preventing cross-contamination while enabling fair comparison.

```
agent_benchmark/
â”œâ”€â”€ dspy/             # DSPy framework implementation [PRIORITY 1]
â”œâ”€â”€ pocketflow/       # PocketFlow framework implementation [PRIORITY 2]
â”œâ”€â”€ crewai/           # CrewAI framework implementation [PRIORITY 3]
â”œâ”€â”€ google_adk/       # Google ADK framework implementation [PRIORITY 4]
â”œâ”€â”€ pydantic_ai/      # Pydantic AI framework implementation [PRIORITY 5]
â”œâ”€â”€ shared_datasets/  # Common test data across frameworks
â”œâ”€â”€ evaluation/       # Cross-framework evaluation tools
â””â”€â”€ docs/            # Project documentation
```

## ğŸ“Š Framework Comparison Matrix

| Framework | Multi-Agent | Type Safety | Learning | Web Search | RAG Support | Complexity | Priority |
|-----------|-------------|-------------|----------|------------|-------------|------------|----------|
| DSPy | âš ï¸ Limited | âœ… Good | âœ… Excellent | âš ï¸ Basic | âœ… Good | High | ğŸ¥‡ 1st |
| PocketFlow | âœ… Good | âš ï¸ Basic | âš ï¸ Basic | âœ… Excellent | âš ï¸ Basic | Medium | ğŸ¥ˆ 2nd |
| CrewAI | âœ… Excellent | âš ï¸ Basic | âŒ Limited | âœ… Good | âœ… Good | Medium | ğŸ¥‰ 3rd |
| Google ADK | âœ… Good | âœ… Good | âš ï¸ Basic | âœ… Good | âœ… Good | Medium | 4th |
| Pydantic AI | âš ï¸ Limited | âœ… Excellent | âŒ Limited | âš ï¸ Basic | âš ï¸ Basic | Low | 5th |

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- UV package manager
- Docker and Docker Compose
- Git

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/your-org/agent_benchmark.git
   cd agent_benchmark
   ```

2. **Choose a framework to explore** (in priority order)
   ```bash
   cd dspy/        # Priority 1: Programming framework for LMs
   cd pocketflow/  # Priority 2: Nested directed graph framework
   cd crewai/      # Priority 3: Multi-agent orchestration
   cd google_adk/  # Priority 4: Google's Agent Development Kit
   cd pydantic_ai/ # Priority 5: Type-safe agent framework
   ```

3. **Install dependencies**
   ```bash
   uv sync
   ```

4. **Start infrastructure**
   ```bash
   docker-compose up -d
   ```

5. **Run a use case**
   ```bash
   cd usecase1_qa/
   uv run python main.py
   ```

### Running Evaluations

```bash
# Run all frameworks on all use cases
python evaluation/benchmark_runner.py --frameworks all --use-cases all

# Run specific framework (start with highest priority)
python evaluation/benchmark_runner.py --frameworks dspy --use-cases all

# Generate comparison report
python evaluation/report_generator.py
```

## ğŸ“‹ Evaluation Use Cases

### Use Case 1: Question & Answer System
Simple Q&A implementation testing basic agent capabilities.

### Use Case 2: Simple RAG (Retrieval-Augmented Generation)
Document retrieval and answer generation with vector databases.

### Use Case 3: Agentic RAG
Advanced RAG with multiple specialized agents for research and synthesis.

### Use Case 4: Web Search Integration
Real-time information retrieval and fact verification.

### Use Case 5: Multi-Agent Collaboration
Complex workflows requiring agent coordination and specialization.

### Use Case 6: Advanced Agentic Use Cases
Framework-specific advanced capabilities and optimizations.

## ğŸ“ˆ Evaluation Metrics

### Quality Metrics
- **Accuracy**: Correctness against ground truth
- **Relevance**: Response appropriateness to queries
- **Completeness**: Coverage of required information
- **Coherence**: Logical consistency and flow

### Performance Metrics
- **Execution Time**: Use case completion speed
- **Resource Usage**: Memory and CPU consumption
- **API Costs**: LLM and service expenses
- **Scalability**: Performance under load

### Framework-Specific Metrics
- **Ease of Implementation**: Development complexity
- **Code Maintainability**: Framework abstractions
- **Documentation Quality**: Available resources
- **Community Support**: Ecosystem maturity

## ğŸ”§ Development Setup

### Framework Development

Each framework maintains its own development environment:

```bash
# Navigate to framework directory (start with highest priority)
cd dspy/

# Install dependencies
uv sync

# Start development infrastructure
docker-compose up -d

# Run tests
uv run pytest

# Run specific use case
cd usecase1_qa/
uv run python main.py
```

### Adding New Use Cases

1. Create task directory in each framework
2. Implement standardized task interface
3. Add evaluation metrics
4. Update benchmark configurations
5. Test across all frameworks

## ğŸ“š Documentation

- [Architecture Guide](ARCHITECTURE.md) - Detailed system design
- [Getting Started](GETTING_STARTED.md) - Quick setup guide
- [Framework Guides](docs/) - Framework-specific documentation
- [API Reference](docs/api/) - Code documentation
- [Evaluation Guide](docs/evaluation/) - Metrics and benchmarking

## ğŸ› ï¸ Development Standards

This project follows modern Python development practices:

- Python 3.11+ with comprehensive type hints
- UV for dependency management and project configuration
- Pydantic for data validation and modeling
- Framework isolation with dedicated infrastructure
- Comprehensive docstrings and code documentation

## ğŸ™ Acknowledgments

- Framework maintainers and communities
- Open source contributors
- Research institutions and papers
- Beta testers and early adopters

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/your-org/agent_benchmark/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-org/agent_benchmark/discussions)
- **Documentation**: [Project Wiki](https://github.com/your-org/agent_benchmark/wiki)
- **Email**: support@your-org.com

---

**Status**: ğŸš§ In Development | **Version**: 1.0.0 | **Last Updated**: December 2024