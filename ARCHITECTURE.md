# Architecture Guide: AI Agent Frameworks Comparison

## ðŸ—ï¸ System Architecture Overview

The AI Agent Frameworks Comparison Project employs a **framework-isolated architecture** designed to ensure fair, unbiased comparison while maintaining operational consistency across evaluations.

## ðŸŽ¯ Design Principles

### 1. Complete Framework Isolation
- **Independent Dependencies**: Each framework maintains its own `pyproject.toml` and dependency ecosystem
- **Isolated Infrastructure**: Dedicated Qdrant, Langfuse, and MCP server instances per framework
- **Separate Environments**: Framework-specific environment variables and configurations
- **Port Isolation**: Non-conflicting port assignments prevent service interference

### 2. Standardized Evaluation Interface
- **Common Task Interface**: All frameworks implement identical task execution patterns
- **Unified Metrics Collection**: Consistent performance and quality measurement
- **Shared Test Data**: Common datasets ensure fair comparison across frameworks
- **Standardized Output Format**: Uniform result structures for analysis

### 3. Operational Consistency
- **Shared Infrastructure Templates**: Common Docker Compose patterns customized per framework
- **Consistent Monitoring**: Unified observability through Langfuse integration
- **Standard Development Workflow**: Common patterns for development and testing

## ðŸ¢ Directory Structure Design

```
agent_benchmark/
â”œâ”€â”€ ðŸ“ Framework Directories (Isolated)
â”‚   â”œâ”€â”€ crewai/                    # CrewAI implementation
â”‚   â”œâ”€â”€ dspy/                      # DSPy implementation
â”‚   â”œâ”€â”€ pocketflow/                # PocketFlow implementation
â”‚   â”œâ”€â”€ google_adk/                # Google ADK implementation
â”‚   â””â”€â”€ pydantic_ai/               # Pydantic AI implementation
â”‚
â”œâ”€â”€ ðŸ“ Shared Components (Cross-Framework)
â”‚   â”œâ”€â”€ shared_datasets/           # Common test data
â”‚   â”œâ”€â”€ evaluation/                # Evaluation framework
â”‚   â”œâ”€â”€ shared_infrastructure/     # Docker templates
â”‚   â””â”€â”€ docs/                      # Documentation
â”‚
â””â”€â”€ ðŸ“ Project Management
    â”œâ”€â”€ .ai/                       # AI assistant context
    â”œâ”€â”€ README.md                  # Project overview
    â”œâ”€â”€ ARCHITECTURE.md            # This document
    â””â”€â”€ GETTING_STARTED.md         # Quick start guide
```

## ðŸ”§ Framework-Specific Architecture

Each framework directory follows a consistent internal structure:

```
{framework_name}/
â”œâ”€â”€ pyproject.toml                 # Framework-specific dependencies
â”œâ”€â”€ uv.lock                        # Locked dependency versions
â”œâ”€â”€ docker-compose.yaml            # Infrastructure stack
â”œâ”€â”€ .env                           # Environment configuration
â”œâ”€â”€ shared/                        # Framework-internal shared components
â”‚   â”œâ”€â”€ config.py                  # Configuration management
â”‚   â”œâ”€â”€ database.py                # Qdrant integration
â”‚   â”œâ”€â”€ tracing.py                 # Langfuse observability
â”‚   â””â”€â”€ mcp_client.py              # MCP protocol client
â””â”€â”€ task{n}_{name}/                # Individual task implementations
    â”œâ”€â”€ main.py                    # Task entry point
    â”œâ”€â”€ {framework_specific}.py    # Framework implementation
    â””â”€â”€ evaluation.py              # Task-specific metrics
```

## ðŸ”„ Data Flow Architecture

### 1. Task Execution Flow
```mermaid
graph TD
    A[Input Data] --> B[Framework Task Implementation]
    B --> C[Framework-Specific Processing]
    C --> D[LLM API Calls]
    C --> E[Vector Database Operations]
    C --> F[MCP Tool Interactions]
    D --> G[Langfuse Tracing]
    E --> G
    F --> G
    G --> H[Task Result]
    H --> I[Evaluation Metrics]
    I --> J[Comparative Analysis]
```

### 2. Infrastructure Communication
```mermaid
graph LR
    A[Framework Task] --> B[Qdrant Vector DB]
    A --> C[Langfuse Observability]
    A --> D[MCP Server]
    A --> E[External APIs]
    B --> F[Vector Storage]
    C --> G[Trace Data]
    D --> H[Tool Execution]
    E --> I[LLM Responses]
```

## ðŸ›¡ï¸ Isolation Mechanisms

### 1. Dependency Isolation
- **UV Virtual Environments**: Each framework uses isolated Python environments
- **Version Locking**: `uv.lock` files ensure reproducible builds
- **Conflict Prevention**: No shared dependencies between frameworks

### 2. Infrastructure Isolation
- **Container Naming**: Framework-prefixed container names prevent conflicts
- **Network Isolation**: Dedicated Docker networks per framework
- **Volume Separation**: Framework-specific data persistence
- **Port Management**: Non-overlapping port ranges

### 3. Data Isolation
- **Vector Collections**: Framework-specific Qdrant collections
- **Trace Separation**: Isolated Langfuse projects per framework
- **Environment Variables**: Framework-specific configuration

## ðŸ“Š Evaluation Architecture

### 1. Metrics Collection
```python
# Standardized evaluation interface
class BaseEvaluator(ABC):
    def evaluate_response_quality(self, expected, actual) -> Dict[str, float]
    def measure_performance(self, execution_func) -> Dict[str, float]
    def execute_with_monitoring(self, task_name, execution_func, input_data) -> TaskResult
```

### 2. Comparative Analysis
- **Unified Result Format**: `TaskResult` model ensures consistent data structure
- **Aggregate Metrics**: Cross-framework performance comparison
- **Statistical Analysis**: Significance testing and confidence intervals

### 3. Reporting System
- **Automated Reports**: Generated comparison dashboards and summaries
- **Multiple Formats**: HTML, PDF, CSV, and JSON outputs
- **Interactive Visualizations**: Framework performance comparisons

## ðŸ”Œ Integration Points

### 1. Shared Datasets
- **Common Test Data**: Identical inputs across all frameworks
- **Ground Truth**: Expected outputs for quality evaluation
- **Metadata**: Dataset descriptions and evaluation criteria

### 2. Evaluation Framework
- **Base Classes**: Abstract interfaces for consistent implementation
- **Metric Calculators**: Standardized quality and performance measurement
- **Report Generators**: Automated analysis and visualization

### 3. Infrastructure Templates
- **Docker Compose Templates**: Customizable infrastructure patterns
- **Environment Templates**: Standard configuration patterns
- **Deployment Scripts**: Consistent setup and teardown procedures

## ðŸš€ Scalability Considerations

### 1. Horizontal Scaling
- **Framework Parallelization**: Independent framework execution
- **Task Distribution**: Parallel task execution within frameworks
- **Resource Allocation**: Configurable resource limits per framework

### 2. Performance Optimization
- **Caching Strategies**: Shared embeddings and computed results
- **Resource Monitoring**: Real-time performance tracking
- **Load Balancing**: Distributed execution across infrastructure

### 3. Extensibility
- **Plugin Architecture**: Easy addition of new frameworks
- **Task Templates**: Standardized patterns for new task types
- **Metric Extensions**: Pluggable evaluation metrics

## ðŸ”’ Security Architecture

### 1. API Key Management
- **Environment Isolation**: Framework-specific API key storage
- **Secret Management**: Secure handling of sensitive credentials
- **Access Control**: Limited scope API permissions

### 2. Network Security
- **Container Isolation**: Restricted network communication
- **Port Management**: Minimal exposed ports
- **Data Encryption**: Secure data transmission and storage

### 3. Audit Trail
- **Comprehensive Logging**: All operations tracked and logged
- **Trace Correlation**: End-to-end request tracking
- **Security Monitoring**: Anomaly detection and alerting

## ðŸ”§ Development Architecture

### 1. Development Workflow
- **Framework-Specific Development**: Independent development environments
- **Shared Tooling**: Common development utilities and scripts
- **Testing Strategy**: Unit, integration, and end-to-end testing

### 2. Quality Assurance
- **Code Standards**: Consistent coding patterns and documentation
- **Automated Testing**: Comprehensive test coverage
- **Performance Benchmarking**: Continuous performance monitoring

### 3. Deployment Strategy
- **Environment Parity**: Consistent development and production environments
- **Infrastructure as Code**: Reproducible infrastructure deployment
- **Monitoring Integration**: Production observability and alerting

This architecture ensures fair, comprehensive comparison of AI agent frameworks while maintaining operational excellence and development efficiency.
